# RAG (Retrieval-Augmented Generation) Project

This is my first end-to-end Retrieval-Augmented Generation (RAG) application, built with open-source tools. The project demonstrates how to connect a **Vector Database (ChromaDB)** with a **Transformer-based LLM (Flan-T5 / LLaMA)** to answer questions grounded in custom documents.

---

## ğŸ“Œ Features
- Load and split documents (PDFs, HR policies, pension docs, university brochures, etc.)
- Generate sentence embeddings using Hugging Face **sentence-transformers**
- Store and query vectors in **ChromaDB**
- Use **Flan-T5 (open)** or **LLaMA (on approval)** for natural language generation
- Interactive **Streamlit UI** for querying
- RAG pipeline that ensures **context-aware answers** and reduces hallucination

---

## ğŸ—ï¸ Tech Stack
- **Python 3.11**
- **Hugging Face Transformers** (Flan-T5, LLaMA)
- **Sentence Transformers**
- **ChromaDB** (Vector Database)
- **LangChain** (retrieval orchestration)
- **Streamlit** (frontend UI)

---

## ğŸ“‚ Project Structure
```
RAG-Project/
â”‚â”€â”€ data/                  # PDF documents
â”‚â”€â”€ db/                    # ChromaDB persistence (created on runtime)
â”‚â”€â”€ main.py                # Calls other modules systematically
â”‚â”€â”€ load.py                # Loads PDFs from â€œ/dataâ€ folder
â”‚â”€â”€ split.py               # Splits documents and chunks them
â”‚â”€â”€ embed.py               # Embeds chunks and loads them into the vector database
â”‚â”€â”€ generation.py          # Retrieval + LLM generation
â”‚â”€â”€ requirements.txt       # Python dependencies
â”‚â”€â”€ README.md              # Project documentation
```

---

## âš¡ Setup Instructions
```bash
# Clone repo
git clone https://github.com/savarbharadwaj/rag-project.git
cd rag-project

# Create virtual environment
python3 -m venv .venv

# Activate environment
# Mac/Linux
source .venv/bin/activate
# Windows
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

- Add your documents: place PDFs in the **data/** folder  
- Run the Streamlit app:
```bash
streamlit run main.py
```

---

## ğŸš€ Usage
1. Enter a query in the Streamlit app  
2. The system retrieves relevant chunks from **ChromaDB**  
3. The **LLM (Flan-T5 / LLaMA)** answers using only the provided context  
4. If context is missing â†’ the model responds with *"I donâ€™t know"*  

---

## ğŸ”® Future Improvements
- Integrate **LLaMA-2** for stronger answers (GPU required)  
- Support more file types (Word, TXT)  
- Add a **chat-style UI with history**  
- Deploy on cloud (**Streamlit Cloud / Hugging Face Spaces**)  

---

## ğŸ‘¤ Author
**Savar Bharadwaj**  
- ğŸŒ Passionate about AI/ML, Data Engineering, and Generative AI  
- ğŸ’¼ Currently expanding skills in **AI Engineering**  
